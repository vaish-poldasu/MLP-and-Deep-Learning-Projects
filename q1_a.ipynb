{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 MLP Multi-Class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 - Original Train Size: 151241, Reduced Train Size: 15124\n",
      "Fold-1 - Original Test Size: 16992, Reduced Test Size: 8496\n",
      "Fold-2 - Original Train Size: 151288, Reduced Train Size: 15129\n",
      "Fold-2 - Original Test Size: 16945, Reduced Test Size: 8472\n",
      "Fold-3 - Original Train Size: 151320, Reduced Train Size: 15132\n",
      "Fold-3 - Original Test Size: 16913, Reduced Test Size: 8456\n",
      "Fold-4 - Original Train Size: 151358, Reduced Train Size: 15136\n",
      "Fold-4 - Original Test Size: 16875, Reduced Test Size: 8438\n",
      "Fold-5 - Original Train Size: 151395, Reduced Train Size: 15140\n",
      "Fold-5 - Original Test Size: 16838, Reduced Test Size: 8419\n",
      "Fold-6 - Original Train Size: 151423, Reduced Train Size: 15142\n",
      "Fold-6 - Original Test Size: 16810, Reduced Test Size: 8405\n",
      "Fold-7 - Original Train Size: 151464, Reduced Train Size: 15146\n",
      "Fold-7 - Original Test Size: 16769, Reduced Test Size: 8384\n",
      "Fold-8 - Original Train Size: 151501, Reduced Train Size: 15150\n",
      "Fold-8 - Original Test Size: 16732, Reduced Test Size: 8366\n",
      "Fold-9 - Original Train Size: 151535, Reduced Train Size: 15154\n",
      "Fold-9 - Original Test Size: 16698, Reduced Test Size: 8349\n",
      "Fold-10 - Original Train Size: 151572, Reduced Train Size: 15157\n",
      "Fold-10 - Original Test Size: 16661, Reduced Test Size: 8330\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# reducing dataset size by 50%\n",
    "for fold in range(1, 11):\n",
    "    train_file_path = f'Symbols/classification-task/fold-{fold}/train.csv'\n",
    "    test_file_path = f'Symbols/classification-task/fold-{fold}/test.csv'\n",
    " \n",
    "    if os.path.exists(train_file_path) and os.path.exists(test_file_path):\n",
    "        train_df = pd.read_csv(train_file_path)\n",
    "        test_df = pd.read_csv(test_file_path)\n",
    "        \n",
    "        #sampling 50% of the dataset\n",
    "        train_df_reduced = train_df.sample(frac=0.1, random_state=42)  \n",
    "        test_df_reduced = test_df.sample(frac=0.5, random_state=42)  \n",
    "\n",
    "        train_reduced_file_path = f'Symbols/classification-task/fold-{fold}/train_reduced.csv'\n",
    "        test_reduced_file_path = f'Symbols/classification-task/fold-{fold}/test_reduced.csv'\n",
    "        \n",
    "        train_df_reduced.to_csv(train_reduced_file_path, index=False)\n",
    "        test_df_reduced.to_csv(test_reduced_file_path, index=False)\n",
    "\n",
    "        print(f\"Fold-{fold} - Original Train Size: {len(train_df)}, Reduced Train Size: {len(train_df_reduced)}\")\n",
    "        print(f\"Fold-{fold} - Original Test Size: {len(test_df)}, Reduced Test Size: {len(test_df_reduced)}\")\n",
    "    else:\n",
    "        print(f\"Files for fold-{fold} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_data(csv_file_path, image_folder):\n",
    "    data_df = pd.read_csv(csv_file_path)\n",
    "    images = []\n",
    "    labels = []\n",
    "    base_image_path = os.path.abspath(image_folder)\n",
    "\n",
    "    for _, row in data_df.iterrows():\n",
    "        image_path = row['path']\n",
    "        symbol_id = row['symbol_id'] \n",
    "        if image_path.startswith(\"../../images/\"):\n",
    "            image_path = os.path.join(base_image_path, image_path[6:]) \n",
    "  \n",
    "        full_image_path = os.path.normpath(image_path)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(full_image_path).convert('L')  # 'L' for grayscale\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {full_image_path}: {e}\")\n",
    "            continue\n",
    "       \n",
    "        image = image.resize((32, 32))\n",
    "        image_array = np.array(image) / 255.0  # Normalizing the pixel values to [0, 1]\n",
    "        flattened_image = image_array.flatten()\n",
    "        \n",
    "        images.append(flattened_image)\n",
    "        labels.append(symbol_id)\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def encode_labels(symbol_ids):\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(symbol_ids) \n",
    "    return encoded_labels, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes (symbols): 369\n",
      "Class names: [  31   32   33   34   35   36   37   38   39   40   41   42   43   44\n",
      "   45   46   47   48   49   50   51   52   53   54   55   56   59   70\n",
      "   71   72   73   74   75   76   77   78   79   81   82   87   88   89\n",
      "   90   91   92   93   94   95   96   97   98   99  100  101  102  103\n",
      "  104  105  106  107  108  110  111  112  113  114  115  116  117  150\n",
      "  151  152  153  154  155  156  157  158  159  160  161  162  163  164\n",
      "  165  166  167  168  169  170  171  174  175  176  177  178  179  180\n",
      "  181  182  183  184  185  186  187  188  189  190  191  192  193  194\n",
      "  195  196  197  254  257  259  260  261  262  263  264  265  266  267\n",
      "  268  269  508  510  511  512  513  514  517  520  521  523  524  526\n",
      "  527  528  529  530  531  532  533  534  535  536  537  538  539  540\n",
      "  541  542  544  549  550  553  555  562  564  574  577  582  583  584\n",
      "  591  595  600  601  603  604  605  607  608  609  610  611  612  613\n",
      "  614  615  616  617  618  620  621  622  630  631  634  635  636  639\n",
      "  640  644  647  650  661  671  678  679  683  684  698  711  712  713\n",
      "  716  728  739  741  743  748  751  753  756  757  758  759  761  762\n",
      "  763  764  765  767  768  770  771  775  777  778  783  785  786  788\n",
      "  791  792  801  809  812  817  822  823  827  837  838  881  882  884\n",
      "  885  886  887  888  889  890  891  892  894  901  912  913  914  915\n",
      "  916  917  918  919  920  921  922  923  924  934  936  941  943  944\n",
      "  945  946  947  948  949  950  951  953  956  957  958  959  960  965\n",
      "  968  971  972  973  974  977  992  993  994  995  996  997  998  999\n",
      " 1000 1004 1005 1006 1007 1008 1010 1011 1012 1013 1016 1018 1019 1031\n",
      " 1037 1042 1045 1046 1051 1053 1062 1064 1065 1066 1074 1075 1077 1078\n",
      " 1079 1080 1082 1086 1090 1093 1101 1102 1103 1111 1112 1115 1116 1117\n",
      " 1168 1169 1177 1184 1185 1187 1314 1315 1316 1317 1369 1371 1374 1382\n",
      " 1385 1394 1395 1396 1400]\n"
     ]
    }
   ],
   "source": [
    "test_images, test_labels = load_data('Symbols/classification-task/fold-1/test_reduced.csv')\n",
    "\n",
    "encoded_labels, label_encoder = encode_labels(test_labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Number of unique classes (symbols):\", num_classes)\n",
    "print(\"Class names:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layers, activation_function='relu', learning_rate=0.01, optimizer='sgd', batch_size=None, device='cpu'):\n",
    "        self.layers = layers  \n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_function = activation_function\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size \n",
    "        self.num_classes = layers[-1] \n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(1, len(layers)):\n",
    "            W = torch.randn(layers[i], layers[i-1], device=self.device) * 0.1\n",
    "            b = torch.zeros(layers[i], 1, device=self.device)\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def activation(self, z, derivative=False):\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            if derivative:\n",
    "                return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "            return self.sigmoid(z)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            if derivative:\n",
    "                return 1 - torch.tanh(z)**2\n",
    "            return torch.tanh(z)\n",
    "        elif self.activation_function == 'relu':\n",
    "            if derivative:\n",
    "                return (z > 0).float()\n",
    "            return torch.maximum(torch.tensor(0.0, device=self.device), z)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        for i in range(len(self.layers) - 2):\n",
    "            z = torch.mm(self.weights[i], activations[-1]) + self.biases[i]\n",
    "            a = self.activation(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Final layerwith softmax activation \n",
    "        z_output = torch.mm(self.weights[-1], activations[-1]) + self.biases[-1]\n",
    "        output = F.softmax(z_output, dim=0) \n",
    "        activations.append(output)\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def backward(self, X, y, activations):\n",
    "        m = X.size(1)\n",
    "        dz = activations[-1] - y\n",
    "        dW = torch.mm(dz, activations[-2].T) / m\n",
    "        db = torch.sum(dz, dim=1, keepdim=True) / m\n",
    "        gradients = [(dW, db)]\n",
    "\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            dz = torch.mm(self.weights[i].T, dz) * self.activation(torch.mm(self.weights[i-1], activations[i-1]) + self.biases[i-1], derivative=True)\n",
    "            dW = torch.mm(dz, activations[i-1].T) / m\n",
    "            db = torch.sum(dz, dim=1, keepdim=True) / m\n",
    "            gradients.append((dW, db))\n",
    "\n",
    "        gradients.reverse()\n",
    "        return gradients\n",
    "\n",
    "    def update_parameters(self, gradients, batch_size=None):\n",
    "        for i in range(len(self.weights)):\n",
    "            dW, db = gradients[i]\n",
    "            if self.optimizer == 'sgd':\n",
    "                self.weights[i] -= self.learning_rate * dW\n",
    "                self.biases[i] -= self.learning_rate * db\n",
    "            elif self.optimizer == 'batch':\n",
    "                self.weights[i] -= self.learning_rate * dW\n",
    "                self.biases[i] -= self.learning_rate * db\n",
    "            elif self.optimizer == 'mini-batch':\n",
    "                self.weights[i] -= (self.learning_rate / batch_size) * dW\n",
    "                self.biases[i] -= (self.learning_rate / batch_size) * db\n",
    "    \n",
    "    def to_one_hot(self,y,num_classes):\n",
    "        y_one_hot = torch.zeros(num_classes,y.size(0),device=self.device)\n",
    "        for i in range(y.size(0)):\n",
    "            y_one_hot[y[i], i] = 1\n",
    "        return y_one_hot\n",
    "    \n",
    "    def train(self, X, y, epochs=100):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=self.device).T\n",
    "        y = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        unique_classes = torch.unique(y)\n",
    "        class_mapping = {cls.item(): i for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "        y_mapped = torch.tensor([class_mapping[label.item()] for label in y], \n",
    "                               dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # Convert to one-hot encoding\n",
    "        y_one_hot = self.to_one_hot(y_mapped, len(unique_classes))\n",
    "        self.class_mapping = class_mapping\n",
    "        self.inverse_mapping = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "        m = X.size(1)\n",
    "        \n",
    "        if self.optimizer == 'mini-batch' and self.batch_size is None:\n",
    "            raise ValueError(\"Batch size must be provided for mini-batch gradient descent.\")\n",
    "        \n",
    "        batch_size = self.batch_size if self.optimizer == 'mini-batch' else m\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            if self.optimizer == 'mini-batch':\n",
    "                indices = torch.randperm(m)\n",
    "                X = X[:, indices]\n",
    "                y_one_hot = y_one_hot[:, indices]\n",
    "            \n",
    "            for i in range(0, m, batch_size):\n",
    "                end = min(i + batch_size, m)\n",
    "                batch_X = X[:, i:end]\n",
    "                batch_y = y_one_hot[:, i:end]\n",
    "                \n",
    "                # Forward pass\n",
    "                activations = self.forward(batch_X)\n",
    "                \n",
    "                # Backward pass\n",
    "                gradients = self.backward(batch_X, batch_y, activations)\n",
    "                \n",
    "                #parameters\n",
    "                self.update_parameters(gradients)\n",
    "                \n",
    "                #loss (cross-entropy)\n",
    "                epsilon = 1e-15  # Small constant to avoid log(0)\n",
    "                predictions = torch.clamp(activations[-1], epsilon, 1.0 - epsilon)\n",
    "                loss = -torch.sum(batch_y * torch.log(predictions)) / batch_X.size(1)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                avg_loss = total_loss / (m // batch_size + (1 if m % batch_size != 0 else 0))\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=self.device).T\n",
    "        activations = self.forward(X)\n",
    "        predictions = activations[-1]\n",
    "        return predictions.cpu().detach().numpy()\n",
    "    \n",
    "    def predict_classes(self,X):\n",
    "        probs = self.predict(X)\n",
    "        pred_indices = np.argmax(probs, axis=0)\n",
    "        if hasattr(self, 'inverse_mapping'):\n",
    "            return np.array([self.inverse_mapping[idx] for idx in pred_indices])\n",
    "        else:\n",
    "            return pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n"
     ]
    }
   ],
   "source": [
    "base_path = 'Symbols'  \n",
    "symbols_df = pd.read_csv('symbols.csv')\n",
    "num_classes = len(symbols_df)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold: fold-1\n",
      "Epoch 1/100, Loss: 26.1099\n",
      "Epoch 10/100, Loss: 19.3489\n",
      "Epoch 20/100, Loss: 16.2964\n",
      "Epoch 30/100, Loss: 14.4973\n",
      "Epoch 40/100, Loss: 13.1707\n",
      "Epoch 50/100, Loss: 12.1094\n",
      "Epoch 60/100, Loss: 11.2335\n",
      "Epoch 70/100, Loss: 10.4967\n",
      "Epoch 80/100, Loss: 9.8662\n",
      "Epoch 90/100, Loss: 9.3206\n",
      "Epoch 100/100, Loss: 8.8438\n",
      "Fold fold-1: Accuracy = 0.1059\n",
      "Processing fold: fold-2\n",
      "Epoch 1/100, Loss: 27.6660\n",
      "Epoch 10/100, Loss: 19.6984\n",
      "Epoch 20/100, Loss: 16.4604\n",
      "Epoch 30/100, Loss: 14.5755\n",
      "Epoch 40/100, Loss: 13.2443\n",
      "Epoch 50/100, Loss: 12.2081\n",
      "Epoch 60/100, Loss: 11.3593\n",
      "Epoch 70/100, Loss: 10.6428\n",
      "Epoch 80/100, Loss: 10.0256\n",
      "Epoch 90/100, Loss: 9.4886\n",
      "Epoch 100/100, Loss: 9.0161\n",
      "Fold fold-2: Accuracy = 0.1010\n",
      "Processing fold: fold-3\n",
      "Epoch 1/100, Loss: 26.9533\n",
      "Epoch 10/100, Loss: 19.2315\n",
      "Epoch 20/100, Loss: 16.0515\n",
      "Epoch 30/100, Loss: 14.2612\n",
      "Epoch 40/100, Loss: 12.9719\n",
      "Epoch 50/100, Loss: 11.9493\n",
      "Epoch 60/100, Loss: 11.1039\n",
      "Epoch 70/100, Loss: 10.3880\n",
      "Epoch 80/100, Loss: 9.7727\n",
      "Epoch 90/100, Loss: 9.2391\n",
      "Epoch 100/100, Loss: 8.7724\n",
      "Fold fold-3: Accuracy = 0.0922\n",
      "Processing fold: fold-4\n",
      "Epoch 1/100, Loss: 28.1891\n",
      "Epoch 10/100, Loss: 19.5564\n",
      "Epoch 20/100, Loss: 16.2687\n",
      "Epoch 30/100, Loss: 14.3605\n",
      "Epoch 40/100, Loss: 12.9954\n",
      "Epoch 50/100, Loss: 11.9414\n",
      "Epoch 60/100, Loss: 11.0866\n",
      "Epoch 70/100, Loss: 10.3688\n",
      "Epoch 80/100, Loss: 9.7554\n",
      "Epoch 90/100, Loss: 9.2255\n",
      "Epoch 100/100, Loss: 8.7629\n",
      "Fold fold-4: Accuracy = 0.1068\n",
      "Processing fold: fold-5\n",
      "Epoch 1/100, Loss: 26.0066\n",
      "Epoch 10/100, Loss: 18.8859\n",
      "Epoch 20/100, Loss: 15.9754\n",
      "Epoch 30/100, Loss: 14.1680\n",
      "Epoch 40/100, Loss: 12.8642\n",
      "Epoch 50/100, Loss: 11.8420\n",
      "Epoch 60/100, Loss: 11.0044\n",
      "Epoch 70/100, Loss: 10.3004\n",
      "Epoch 80/100, Loss: 9.6985\n",
      "Epoch 90/100, Loss: 9.1778\n",
      "Epoch 100/100, Loss: 8.7239\n",
      "Fold fold-5: Accuracy = 0.0976\n",
      "Processing fold: fold-6\n",
      "Epoch 1/100, Loss: 24.9912\n",
      "Epoch 10/100, Loss: 18.3119\n",
      "Epoch 20/100, Loss: 15.4298\n",
      "Epoch 30/100, Loss: 13.6985\n",
      "Epoch 40/100, Loss: 12.4326\n",
      "Epoch 50/100, Loss: 11.4338\n",
      "Epoch 60/100, Loss: 10.6161\n",
      "Epoch 70/100, Loss: 9.9309\n",
      "Epoch 80/100, Loss: 9.3476\n",
      "Epoch 90/100, Loss: 8.8445\n",
      "Epoch 100/100, Loss: 8.4070\n",
      "Fold fold-6: Accuracy = 0.0872\n",
      "Processing fold: fold-7\n",
      "Epoch 1/100, Loss: 26.6035\n",
      "Epoch 10/100, Loss: 18.9043\n",
      "Epoch 20/100, Loss: 15.8970\n",
      "Epoch 30/100, Loss: 14.0674\n",
      "Epoch 40/100, Loss: 12.7585\n",
      "Epoch 50/100, Loss: 11.7520\n",
      "Epoch 60/100, Loss: 10.9322\n",
      "Epoch 70/100, Loss: 10.2439\n",
      "Epoch 80/100, Loss: 9.6546\n",
      "Epoch 90/100, Loss: 9.1427\n",
      "Epoch 100/100, Loss: 8.6941\n",
      "Fold fold-7: Accuracy = 0.0990\n",
      "Processing fold: fold-8\n",
      "Epoch 1/100, Loss: 25.5063\n",
      "Epoch 10/100, Loss: 18.9054\n",
      "Epoch 20/100, Loss: 15.9847\n",
      "Epoch 30/100, Loss: 14.1661\n",
      "Epoch 40/100, Loss: 12.8250\n",
      "Epoch 50/100, Loss: 11.7726\n",
      "Epoch 60/100, Loss: 10.9161\n",
      "Epoch 70/100, Loss: 10.2007\n",
      "Epoch 80/100, Loss: 9.5916\n",
      "Epoch 90/100, Loss: 9.0663\n",
      "Epoch 100/100, Loss: 8.6095\n",
      "Fold fold-8: Accuracy = 0.1054\n",
      "Processing fold: fold-9\n",
      "Epoch 1/100, Loss: 26.8339\n",
      "Epoch 10/100, Loss: 19.3892\n",
      "Epoch 20/100, Loss: 16.2891\n",
      "Epoch 30/100, Loss: 14.3601\n",
      "Epoch 40/100, Loss: 12.9894\n",
      "Epoch 50/100, Loss: 11.9386\n",
      "Epoch 60/100, Loss: 11.0864\n",
      "Epoch 70/100, Loss: 10.3715\n",
      "Epoch 80/100, Loss: 9.7627\n",
      "Epoch 90/100, Loss: 9.2378\n",
      "Epoch 100/100, Loss: 8.7800\n",
      "Fold fold-9: Accuracy = 0.0979\n",
      "Processing fold: fold-10\n",
      "Epoch 1/100, Loss: 26.0255\n",
      "Epoch 10/100, Loss: 19.3799\n",
      "Epoch 20/100, Loss: 16.4542\n",
      "Epoch 30/100, Loss: 14.5935\n",
      "Epoch 40/100, Loss: 13.2160\n",
      "Epoch 50/100, Loss: 12.1373\n",
      "Epoch 60/100, Loss: 11.2599\n",
      "Epoch 70/100, Loss: 10.5289\n",
      "Epoch 80/100, Loss: 9.9070\n",
      "Epoch 90/100, Loss: 9.3698\n",
      "Epoch 100/100, Loss: 8.9001\n",
      "Fold fold-10: Accuracy = 0.1016\n",
      "Average Accuracy across all 10 folds: 0.0995\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "base_path = 'Symbols/classification-task'\n",
    "image_folder = 'Symbols'\n",
    "\n",
    "folds = [f'fold-{i}' for i in range(1, 11)]\n",
    "\n",
    "symbols_df = pd.read_csv('symbols.csv')\n",
    "num_classes = len(symbols_df)\n",
    "\n",
    "input_size = 32 * 32\n",
    "layers = [input_size, 768, 512, num_classes]\n",
    "activation_function = 'relu'\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "results = []\n",
    "\n",
    "# 10-fold cross-validation\n",
    "for fold in folds:\n",
    "    print(f\"Processing fold: {fold}\")\n",
    "    \n",
    "    #current fold's test set\n",
    "    test_path = os.path.join(base_path, fold, 'test_reduced.csv')\n",
    "    X_test, y_test = load_data(test_path, image_folder)\n",
    "    \n",
    "    # other folds' training data\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    \n",
    "    for other_fold in folds:\n",
    "        if other_fold != fold:  \n",
    "            train_path = os.path.join(base_path, other_fold, 'train_reduced.csv')\n",
    "            X_fold, y_fold = load_data(train_path, image_folder)\n",
    "            X_train_list.append(X_fold)\n",
    "            y_train_list.append(y_fold)\n",
    "    \n",
    "    # Combine all training data\n",
    "    X_train = np.vstack(X_train_list)\n",
    "    y_train = np.concatenate(y_train_list)\n",
    "\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "    X_test_scaled = scaler.transform(X_test_flat)\n",
    "   \n",
    "    model = MLP(\n",
    "        layers=layers,\n",
    "        activation_function=activation_function,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer='batch',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    model.train(X_train_scaled, y_train, epochs=epochs)\n",
    "    \n",
    "    y_pred_classes = model.predict_classes(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    results.append(accuracy)\n",
    "    \n",
    "    print(f'Fold {fold}: Accuracy = {accuracy:.4f}')\n",
    "\n",
    "# average accuracy\n",
    "average_accuracy = sum(results) / len(results)\n",
    "print(f'Average Accuracy across all 10 folds: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold: fold-1\n",
      "Epoch 1/500, Loss: 7.5556\n",
      "Epoch 10/500, Loss: 7.4694\n",
      "Epoch 20/500, Loss: 7.3746\n",
      "Epoch 30/500, Loss: 7.2810\n",
      "Epoch 40/500, Loss: 7.1886\n",
      "Epoch 50/500, Loss: 7.0977\n",
      "Epoch 60/500, Loss: 7.0084\n",
      "Epoch 70/500, Loss: 6.9210\n",
      "Epoch 80/500, Loss: 6.8359\n",
      "Epoch 90/500, Loss: 6.7532\n",
      "Epoch 100/500, Loss: 6.6731\n",
      "Epoch 110/500, Loss: 6.5955\n",
      "Epoch 120/500, Loss: 6.5203\n",
      "Epoch 130/500, Loss: 6.4473\n",
      "Epoch 140/500, Loss: 6.3764\n",
      "Epoch 150/500, Loss: 6.3076\n",
      "Epoch 160/500, Loss: 6.2407\n",
      "Epoch 170/500, Loss: 6.1757\n",
      "Epoch 180/500, Loss: 6.1125\n",
      "Epoch 190/500, Loss: 6.0510\n",
      "Epoch 200/500, Loss: 5.9912\n",
      "Epoch 210/500, Loss: 5.9331\n",
      "Epoch 220/500, Loss: 5.8765\n",
      "Epoch 230/500, Loss: 5.8214\n",
      "Epoch 240/500, Loss: 5.7678\n",
      "Epoch 250/500, Loss: 5.7156\n",
      "Epoch 260/500, Loss: 5.6648\n",
      "Epoch 270/500, Loss: 5.6152\n",
      "Epoch 280/500, Loss: 5.5670\n",
      "Epoch 290/500, Loss: 5.5199\n",
      "Epoch 300/500, Loss: 5.4740\n",
      "Epoch 310/500, Loss: 5.4293\n",
      "Epoch 320/500, Loss: 5.3855\n",
      "Epoch 330/500, Loss: 5.3428\n",
      "Epoch 340/500, Loss: 5.3011\n",
      "Epoch 350/500, Loss: 5.2603\n",
      "Epoch 360/500, Loss: 5.2204\n",
      "Epoch 370/500, Loss: 5.1814\n",
      "Epoch 380/500, Loss: 5.1432\n",
      "Epoch 390/500, Loss: 5.1057\n",
      "Epoch 400/500, Loss: 5.0691\n",
      "Epoch 410/500, Loss: 5.0331\n",
      "Epoch 420/500, Loss: 4.9979\n",
      "Epoch 430/500, Loss: 4.9633\n",
      "Epoch 440/500, Loss: 4.9294\n",
      "Epoch 450/500, Loss: 4.8962\n",
      "Epoch 460/500, Loss: 4.8635\n",
      "Epoch 470/500, Loss: 4.8314\n",
      "Epoch 480/500, Loss: 4.7999\n",
      "Epoch 490/500, Loss: 4.7690\n",
      "Epoch 500/500, Loss: 4.7386\n",
      "Fold fold-1: Accuracy = 0.1921\n",
      "Processing fold: fold-2\n",
      "Epoch 1/500, Loss: 7.6651\n",
      "Epoch 10/500, Loss: 7.5782\n",
      "Epoch 20/500, Loss: 7.4826\n",
      "Epoch 30/500, Loss: 7.3882\n",
      "Epoch 40/500, Loss: 7.2951\n",
      "Epoch 50/500, Loss: 7.2035\n",
      "Epoch 60/500, Loss: 7.1135\n",
      "Epoch 70/500, Loss: 7.0254\n",
      "Epoch 80/500, Loss: 6.9395\n",
      "Epoch 90/500, Loss: 6.8558\n",
      "Epoch 100/500, Loss: 6.7746\n",
      "Epoch 110/500, Loss: 6.6958\n",
      "Epoch 120/500, Loss: 6.6193\n",
      "Epoch 130/500, Loss: 6.5450\n",
      "Epoch 140/500, Loss: 6.4728\n",
      "Epoch 150/500, Loss: 6.4025\n",
      "Epoch 160/500, Loss: 6.3339\n",
      "Epoch 170/500, Loss: 6.2671\n",
      "Epoch 180/500, Loss: 6.2019\n",
      "Epoch 190/500, Loss: 6.1383\n",
      "Epoch 200/500, Loss: 6.0763\n",
      "Epoch 210/500, Loss: 6.0157\n",
      "Epoch 220/500, Loss: 5.9565\n",
      "Epoch 230/500, Loss: 5.8988\n",
      "Epoch 240/500, Loss: 5.8425\n",
      "Epoch 250/500, Loss: 5.7875\n",
      "Epoch 260/500, Loss: 5.7338\n",
      "Epoch 270/500, Loss: 5.6814\n",
      "Epoch 280/500, Loss: 5.6302\n",
      "Epoch 290/500, Loss: 5.5803\n",
      "Epoch 300/500, Loss: 5.5315\n",
      "Epoch 310/500, Loss: 5.4839\n",
      "Epoch 320/500, Loss: 5.4374\n",
      "Epoch 330/500, Loss: 5.3920\n",
      "Epoch 340/500, Loss: 5.3477\n",
      "Epoch 350/500, Loss: 5.3044\n",
      "Epoch 360/500, Loss: 5.2621\n",
      "Epoch 370/500, Loss: 5.2208\n",
      "Epoch 380/500, Loss: 5.1804\n",
      "Epoch 390/500, Loss: 5.1410\n",
      "Epoch 400/500, Loss: 5.1024\n",
      "Epoch 410/500, Loss: 5.0647\n",
      "Epoch 420/500, Loss: 5.0278\n",
      "Epoch 430/500, Loss: 4.9917\n",
      "Epoch 440/500, Loss: 4.9564\n",
      "Epoch 450/500, Loss: 4.9218\n",
      "Epoch 460/500, Loss: 4.8880\n",
      "Epoch 470/500, Loss: 4.8549\n",
      "Epoch 480/500, Loss: 4.8224\n",
      "Epoch 490/500, Loss: 4.7906\n",
      "Epoch 500/500, Loss: 4.7595\n",
      "Fold fold-2: Accuracy = 0.1852\n",
      "Processing fold: fold-3\n",
      "Epoch 1/500, Loss: 7.7333\n",
      "Epoch 10/500, Loss: 7.6448\n",
      "Epoch 20/500, Loss: 7.5474\n",
      "Epoch 30/500, Loss: 7.4512\n",
      "Epoch 40/500, Loss: 7.3562\n",
      "Epoch 50/500, Loss: 7.2623\n",
      "Epoch 60/500, Loss: 7.1698\n",
      "Epoch 70/500, Loss: 7.0789\n",
      "Epoch 80/500, Loss: 6.9896\n",
      "Epoch 90/500, Loss: 6.9023\n",
      "Epoch 100/500, Loss: 6.8173\n",
      "Epoch 110/500, Loss: 6.7346\n",
      "Epoch 120/500, Loss: 6.6542\n",
      "Epoch 130/500, Loss: 6.5762\n",
      "Epoch 140/500, Loss: 6.5003\n",
      "Epoch 150/500, Loss: 6.4266\n",
      "Epoch 160/500, Loss: 6.3548\n",
      "Epoch 170/500, Loss: 6.2849\n",
      "Epoch 180/500, Loss: 6.2169\n",
      "Epoch 190/500, Loss: 6.1507\n",
      "Epoch 200/500, Loss: 6.0863\n",
      "Epoch 210/500, Loss: 6.0236\n",
      "Epoch 220/500, Loss: 5.9626\n",
      "Epoch 230/500, Loss: 5.9033\n",
      "Epoch 240/500, Loss: 5.8456\n",
      "Epoch 250/500, Loss: 5.7895\n",
      "Epoch 260/500, Loss: 5.7348\n",
      "Epoch 270/500, Loss: 5.6816\n",
      "Epoch 280/500, Loss: 5.6298\n",
      "Epoch 290/500, Loss: 5.5793\n",
      "Epoch 300/500, Loss: 5.5302\n",
      "Epoch 310/500, Loss: 5.4823\n",
      "Epoch 320/500, Loss: 5.4355\n",
      "Epoch 330/500, Loss: 5.3900\n",
      "Epoch 340/500, Loss: 5.3456\n",
      "Epoch 350/500, Loss: 5.3022\n",
      "Epoch 360/500, Loss: 5.2599\n",
      "Epoch 370/500, Loss: 5.2186\n",
      "Epoch 380/500, Loss: 5.1783\n",
      "Epoch 390/500, Loss: 5.1389\n",
      "Epoch 400/500, Loss: 5.1004\n",
      "Epoch 410/500, Loss: 5.0627\n",
      "Epoch 420/500, Loss: 5.0259\n",
      "Epoch 430/500, Loss: 4.9899\n",
      "Epoch 440/500, Loss: 4.9547\n",
      "Epoch 450/500, Loss: 4.9203\n",
      "Epoch 460/500, Loss: 4.8865\n",
      "Epoch 470/500, Loss: 4.8535\n",
      "Epoch 480/500, Loss: 4.8211\n",
      "Epoch 490/500, Loss: 4.7894\n",
      "Epoch 500/500, Loss: 4.7583\n",
      "Fold fold-3: Accuracy = 0.1877\n",
      "Processing fold: fold-4\n",
      "Epoch 1/500, Loss: 7.7027\n",
      "Epoch 10/500, Loss: 7.6154\n",
      "Epoch 20/500, Loss: 7.5194\n",
      "Epoch 30/500, Loss: 7.4244\n",
      "Epoch 40/500, Loss: 7.3305\n",
      "Epoch 50/500, Loss: 7.2376\n",
      "Epoch 60/500, Loss: 7.1460\n",
      "Epoch 70/500, Loss: 7.0558\n",
      "Epoch 80/500, Loss: 6.9673\n",
      "Epoch 90/500, Loss: 6.8806\n",
      "Epoch 100/500, Loss: 6.7960\n",
      "Epoch 110/500, Loss: 6.7138\n",
      "Epoch 120/500, Loss: 6.6340\n",
      "Epoch 130/500, Loss: 6.5566\n",
      "Epoch 140/500, Loss: 6.4816\n",
      "Epoch 150/500, Loss: 6.4089\n",
      "Epoch 160/500, Loss: 6.3383\n",
      "Epoch 170/500, Loss: 6.2696\n",
      "Epoch 180/500, Loss: 6.2029\n",
      "Epoch 190/500, Loss: 6.1379\n",
      "Epoch 200/500, Loss: 6.0748\n",
      "Epoch 210/500, Loss: 6.0133\n",
      "Epoch 220/500, Loss: 5.9536\n",
      "Epoch 230/500, Loss: 5.8954\n",
      "Epoch 240/500, Loss: 5.8388\n",
      "Epoch 250/500, Loss: 5.7837\n",
      "Epoch 260/500, Loss: 5.7300\n",
      "Epoch 270/500, Loss: 5.6778\n",
      "Epoch 280/500, Loss: 5.6269\n",
      "Epoch 290/500, Loss: 5.5774\n",
      "Epoch 300/500, Loss: 5.5290\n",
      "Epoch 310/500, Loss: 5.4820\n",
      "Epoch 320/500, Loss: 5.4360\n",
      "Epoch 330/500, Loss: 5.3912\n",
      "Epoch 340/500, Loss: 5.3475\n",
      "Epoch 350/500, Loss: 5.3048\n",
      "Epoch 360/500, Loss: 5.2632\n",
      "Epoch 370/500, Loss: 5.2225\n",
      "Epoch 380/500, Loss: 5.1827\n",
      "Epoch 390/500, Loss: 5.1438\n",
      "Epoch 400/500, Loss: 5.1057\n",
      "Epoch 410/500, Loss: 5.0685\n",
      "Epoch 420/500, Loss: 5.0321\n",
      "Epoch 430/500, Loss: 4.9965\n",
      "Epoch 440/500, Loss: 4.9616\n",
      "Epoch 450/500, Loss: 4.9275\n",
      "Epoch 460/500, Loss: 4.8940\n",
      "Epoch 470/500, Loss: 4.8612\n",
      "Epoch 480/500, Loss: 4.8291\n",
      "Epoch 490/500, Loss: 4.7976\n",
      "Epoch 500/500, Loss: 4.7667\n",
      "Fold fold-4: Accuracy = 0.1919\n",
      "Processing fold: fold-5\n",
      "Epoch 1/500, Loss: 7.6190\n",
      "Epoch 10/500, Loss: 7.5324\n",
      "Epoch 20/500, Loss: 7.4374\n",
      "Epoch 30/500, Loss: 7.3439\n",
      "Epoch 40/500, Loss: 7.2520\n",
      "Epoch 50/500, Loss: 7.1620\n",
      "Epoch 60/500, Loss: 7.0741\n",
      "Epoch 70/500, Loss: 6.9885\n",
      "Epoch 80/500, Loss: 6.9052\n",
      "Epoch 90/500, Loss: 6.8241\n",
      "Epoch 100/500, Loss: 6.7450\n",
      "Epoch 110/500, Loss: 6.6679\n",
      "Epoch 120/500, Loss: 6.5926\n",
      "Epoch 130/500, Loss: 6.5190\n",
      "Epoch 140/500, Loss: 6.4471\n",
      "Epoch 150/500, Loss: 6.3767\n",
      "Epoch 160/500, Loss: 6.3079\n",
      "Epoch 170/500, Loss: 6.2407\n",
      "Epoch 180/500, Loss: 6.1750\n",
      "Epoch 190/500, Loss: 6.1110\n",
      "Epoch 200/500, Loss: 6.0484\n",
      "Epoch 210/500, Loss: 5.9875\n",
      "Epoch 220/500, Loss: 5.9281\n",
      "Epoch 230/500, Loss: 5.8702\n",
      "Epoch 240/500, Loss: 5.8138\n",
      "Epoch 250/500, Loss: 5.7589\n",
      "Epoch 260/500, Loss: 5.7054\n",
      "Epoch 270/500, Loss: 5.6533\n",
      "Epoch 280/500, Loss: 5.6025\n",
      "Epoch 290/500, Loss: 5.5530\n",
      "Epoch 300/500, Loss: 5.5047\n",
      "Epoch 310/500, Loss: 5.4576\n",
      "Epoch 320/500, Loss: 5.4118\n",
      "Epoch 330/500, Loss: 5.3670\n",
      "Epoch 340/500, Loss: 5.3233\n",
      "Epoch 350/500, Loss: 5.2806\n",
      "Epoch 360/500, Loss: 5.2390\n",
      "Epoch 370/500, Loss: 5.1983\n",
      "Epoch 380/500, Loss: 5.1586\n",
      "Epoch 390/500, Loss: 5.1197\n",
      "Epoch 400/500, Loss: 5.0818\n",
      "Epoch 410/500, Loss: 5.0447\n",
      "Epoch 420/500, Loss: 5.0084\n",
      "Epoch 430/500, Loss: 4.9728\n",
      "Epoch 440/500, Loss: 4.9381\n",
      "Epoch 450/500, Loss: 4.9041\n",
      "Epoch 460/500, Loss: 4.8708\n",
      "Epoch 470/500, Loss: 4.8381\n",
      "Epoch 480/500, Loss: 4.8062\n",
      "Epoch 490/500, Loss: 4.7749\n",
      "Epoch 500/500, Loss: 4.7442\n",
      "Fold fold-5: Accuracy = 0.1885\n",
      "Processing fold: fold-6\n",
      "Epoch 1/500, Loss: 7.5643\n",
      "Epoch 10/500, Loss: 7.4774\n",
      "Epoch 20/500, Loss: 7.3819\n",
      "Epoch 30/500, Loss: 7.2878\n",
      "Epoch 40/500, Loss: 7.1952\n",
      "Epoch 50/500, Loss: 7.1043\n",
      "Epoch 60/500, Loss: 7.0155\n",
      "Epoch 70/500, Loss: 6.9291\n",
      "Epoch 80/500, Loss: 6.8453\n",
      "Epoch 90/500, Loss: 6.7642\n",
      "Epoch 100/500, Loss: 6.6857\n",
      "Epoch 110/500, Loss: 6.6097\n",
      "Epoch 120/500, Loss: 6.5360\n",
      "Epoch 130/500, Loss: 6.4644\n",
      "Epoch 140/500, Loss: 6.3949\n",
      "Epoch 150/500, Loss: 6.3272\n",
      "Epoch 160/500, Loss: 6.2613\n",
      "Epoch 170/500, Loss: 6.1970\n",
      "Epoch 180/500, Loss: 6.1344\n",
      "Epoch 190/500, Loss: 6.0733\n",
      "Epoch 200/500, Loss: 6.0137\n",
      "Epoch 210/500, Loss: 5.9556\n",
      "Epoch 220/500, Loss: 5.8989\n",
      "Epoch 230/500, Loss: 5.8436\n",
      "Epoch 240/500, Loss: 5.7896\n",
      "Epoch 250/500, Loss: 5.7369\n",
      "Epoch 260/500, Loss: 5.6855\n",
      "Epoch 270/500, Loss: 5.6354\n",
      "Epoch 280/500, Loss: 5.5865\n",
      "Epoch 290/500, Loss: 5.5387\n",
      "Epoch 300/500, Loss: 5.4921\n",
      "Epoch 310/500, Loss: 5.4465\n",
      "Epoch 320/500, Loss: 5.4021\n",
      "Epoch 330/500, Loss: 5.3587\n",
      "Epoch 340/500, Loss: 5.3163\n",
      "Epoch 350/500, Loss: 5.2748\n",
      "Epoch 360/500, Loss: 5.2343\n",
      "Epoch 370/500, Loss: 5.1947\n",
      "Epoch 380/500, Loss: 5.1560\n",
      "Epoch 390/500, Loss: 5.1182\n",
      "Epoch 400/500, Loss: 5.0812\n",
      "Epoch 410/500, Loss: 5.0450\n",
      "Epoch 420/500, Loss: 5.0096\n",
      "Epoch 430/500, Loss: 4.9749\n",
      "Epoch 440/500, Loss: 4.9409\n",
      "Epoch 450/500, Loss: 4.9077\n",
      "Epoch 460/500, Loss: 4.8751\n",
      "Epoch 470/500, Loss: 4.8433\n",
      "Epoch 480/500, Loss: 4.8120\n",
      "Epoch 490/500, Loss: 4.7814\n",
      "Epoch 500/500, Loss: 4.7514\n",
      "Fold fold-6: Accuracy = 0.1851\n",
      "Processing fold: fold-7\n",
      "Epoch 1/500, Loss: 7.6195\n",
      "Epoch 10/500, Loss: 7.5328\n",
      "Epoch 20/500, Loss: 7.4377\n",
      "Epoch 30/500, Loss: 7.3441\n",
      "Epoch 40/500, Loss: 7.2522\n",
      "Epoch 50/500, Loss: 7.1620\n",
      "Epoch 60/500, Loss: 7.0738\n",
      "Epoch 70/500, Loss: 6.9876\n",
      "Epoch 80/500, Loss: 6.9036\n",
      "Epoch 90/500, Loss: 6.8219\n",
      "Epoch 100/500, Loss: 6.7425\n",
      "Epoch 110/500, Loss: 6.6652\n",
      "Epoch 120/500, Loss: 6.5899\n",
      "Epoch 130/500, Loss: 6.5167\n",
      "Epoch 140/500, Loss: 6.4453\n",
      "Epoch 150/500, Loss: 6.3757\n",
      "Epoch 160/500, Loss: 6.3079\n",
      "Epoch 170/500, Loss: 6.2418\n",
      "Epoch 180/500, Loss: 6.1775\n",
      "Epoch 190/500, Loss: 6.1147\n",
      "Epoch 200/500, Loss: 6.0536\n",
      "Epoch 210/500, Loss: 5.9940\n",
      "Epoch 220/500, Loss: 5.9359\n",
      "Epoch 230/500, Loss: 5.8792\n",
      "Epoch 240/500, Loss: 5.8239\n",
      "Epoch 250/500, Loss: 5.7700\n",
      "Epoch 260/500, Loss: 5.7174\n",
      "Epoch 270/500, Loss: 5.6660\n",
      "Epoch 280/500, Loss: 5.6159\n",
      "Epoch 290/500, Loss: 5.5670\n",
      "Epoch 300/500, Loss: 5.5193\n",
      "Epoch 310/500, Loss: 5.4727\n",
      "Epoch 320/500, Loss: 5.4272\n",
      "Epoch 330/500, Loss: 5.3827\n",
      "Epoch 340/500, Loss: 5.3393\n",
      "Epoch 350/500, Loss: 5.2969\n",
      "Epoch 360/500, Loss: 5.2554\n",
      "Epoch 370/500, Loss: 5.2149\n",
      "Epoch 380/500, Loss: 5.1752\n",
      "Epoch 390/500, Loss: 5.1364\n",
      "Epoch 400/500, Loss: 5.0985\n",
      "Epoch 410/500, Loss: 5.0613\n",
      "Epoch 420/500, Loss: 5.0250\n",
      "Epoch 430/500, Loss: 4.9894\n",
      "Epoch 440/500, Loss: 4.9545\n",
      "Epoch 450/500, Loss: 4.9204\n",
      "Epoch 460/500, Loss: 4.8869\n",
      "Epoch 470/500, Loss: 4.8541\n",
      "Epoch 480/500, Loss: 4.8219\n",
      "Epoch 490/500, Loss: 4.7903\n",
      "Epoch 500/500, Loss: 4.7594\n",
      "Fold fold-7: Accuracy = 0.1997\n",
      "Processing fold: fold-8\n",
      "Epoch 1/500, Loss: 7.5928\n",
      "Epoch 10/500, Loss: 7.5067\n",
      "Epoch 20/500, Loss: 7.4125\n",
      "Epoch 30/500, Loss: 7.3198\n",
      "Epoch 40/500, Loss: 7.2287\n",
      "Epoch 50/500, Loss: 7.1392\n",
      "Epoch 60/500, Loss: 7.0513\n",
      "Epoch 70/500, Loss: 6.9652\n",
      "Epoch 80/500, Loss: 6.8807\n",
      "Epoch 90/500, Loss: 6.7981\n",
      "Epoch 100/500, Loss: 6.7174\n",
      "Epoch 110/500, Loss: 6.6387\n",
      "Epoch 120/500, Loss: 6.5621\n",
      "Epoch 130/500, Loss: 6.4875\n",
      "Epoch 140/500, Loss: 6.4149\n",
      "Epoch 150/500, Loss: 6.3444\n",
      "Epoch 160/500, Loss: 6.2757\n",
      "Epoch 170/500, Loss: 6.2089\n",
      "Epoch 180/500, Loss: 6.1438\n",
      "Epoch 190/500, Loss: 6.0804\n",
      "Epoch 200/500, Loss: 6.0187\n",
      "Epoch 210/500, Loss: 5.9585\n",
      "Epoch 220/500, Loss: 5.8998\n",
      "Epoch 230/500, Loss: 5.8426\n",
      "Epoch 240/500, Loss: 5.7868\n",
      "Epoch 250/500, Loss: 5.7324\n",
      "Epoch 260/500, Loss: 5.6794\n",
      "Epoch 270/500, Loss: 5.6276\n",
      "Epoch 280/500, Loss: 5.5771\n",
      "Epoch 290/500, Loss: 5.5279\n",
      "Epoch 300/500, Loss: 5.4798\n",
      "Epoch 310/500, Loss: 5.4329\n",
      "Epoch 320/500, Loss: 5.3872\n",
      "Epoch 330/500, Loss: 5.3425\n",
      "Epoch 340/500, Loss: 5.2989\n",
      "Epoch 350/500, Loss: 5.2563\n",
      "Epoch 360/500, Loss: 5.2147\n",
      "Epoch 370/500, Loss: 5.1740\n",
      "Epoch 380/500, Loss: 5.1343\n",
      "Epoch 390/500, Loss: 5.0955\n",
      "Epoch 400/500, Loss: 5.0576\n",
      "Epoch 410/500, Loss: 5.0205\n",
      "Epoch 420/500, Loss: 4.9843\n",
      "Epoch 430/500, Loss: 4.9488\n",
      "Epoch 440/500, Loss: 4.9141\n",
      "Epoch 450/500, Loss: 4.8802\n",
      "Epoch 460/500, Loss: 4.8470\n",
      "Epoch 470/500, Loss: 4.8145\n",
      "Epoch 480/500, Loss: 4.7826\n",
      "Epoch 490/500, Loss: 4.7515\n",
      "Epoch 500/500, Loss: 4.7209\n",
      "Fold fold-8: Accuracy = 0.1930\n",
      "Processing fold: fold-9\n",
      "Epoch 1/500, Loss: 7.6486\n",
      "Epoch 10/500, Loss: 7.5619\n",
      "Epoch 20/500, Loss: 7.4665\n",
      "Epoch 30/500, Loss: 7.3722\n",
      "Epoch 40/500, Loss: 7.2791\n",
      "Epoch 50/500, Loss: 7.1874\n",
      "Epoch 60/500, Loss: 7.0973\n",
      "Epoch 70/500, Loss: 7.0091\n",
      "Epoch 80/500, Loss: 6.9230\n",
      "Epoch 90/500, Loss: 6.8390\n",
      "Epoch 100/500, Loss: 6.7572\n",
      "Epoch 110/500, Loss: 6.6776\n",
      "Epoch 120/500, Loss: 6.6001\n",
      "Epoch 130/500, Loss: 6.5247\n",
      "Epoch 140/500, Loss: 6.4513\n",
      "Epoch 150/500, Loss: 6.3798\n",
      "Epoch 160/500, Loss: 6.3102\n",
      "Epoch 170/500, Loss: 6.2424\n",
      "Epoch 180/500, Loss: 6.1764\n",
      "Epoch 190/500, Loss: 6.1123\n",
      "Epoch 200/500, Loss: 6.0499\n",
      "Epoch 210/500, Loss: 5.9892\n",
      "Epoch 220/500, Loss: 5.9301\n",
      "Epoch 230/500, Loss: 5.8727\n",
      "Epoch 240/500, Loss: 5.8169\n",
      "Epoch 250/500, Loss: 5.7625\n",
      "Epoch 260/500, Loss: 5.7096\n",
      "Epoch 270/500, Loss: 5.6581\n",
      "Epoch 280/500, Loss: 5.6080\n",
      "Epoch 290/500, Loss: 5.5591\n",
      "Epoch 300/500, Loss: 5.5115\n",
      "Epoch 310/500, Loss: 5.4650\n",
      "Epoch 320/500, Loss: 5.4197\n",
      "Epoch 330/500, Loss: 5.3755\n",
      "Epoch 340/500, Loss: 5.3324\n",
      "Epoch 350/500, Loss: 5.2902\n",
      "Epoch 360/500, Loss: 5.2490\n",
      "Epoch 370/500, Loss: 5.2088\n",
      "Epoch 380/500, Loss: 5.1694\n",
      "Epoch 390/500, Loss: 5.1310\n",
      "Epoch 400/500, Loss: 5.0933\n",
      "Epoch 410/500, Loss: 5.0565\n",
      "Epoch 420/500, Loss: 5.0204\n",
      "Epoch 430/500, Loss: 4.9851\n",
      "Epoch 440/500, Loss: 4.9505\n",
      "Epoch 450/500, Loss: 4.9166\n",
      "Epoch 460/500, Loss: 4.8834\n",
      "Epoch 470/500, Loss: 4.8509\n",
      "Epoch 480/500, Loss: 4.8189\n",
      "Epoch 490/500, Loss: 4.7877\n",
      "Epoch 500/500, Loss: 4.7570\n",
      "Fold fold-9: Accuracy = 0.1906\n",
      "Processing fold: fold-10\n",
      "Epoch 1/500, Loss: 7.6622\n",
      "Epoch 10/500, Loss: 7.5760\n",
      "Epoch 20/500, Loss: 7.4811\n",
      "Epoch 30/500, Loss: 7.3874\n",
      "Epoch 40/500, Loss: 7.2949\n",
      "Epoch 50/500, Loss: 7.2037\n",
      "Epoch 60/500, Loss: 7.1139\n",
      "Epoch 70/500, Loss: 7.0258\n",
      "Epoch 80/500, Loss: 6.9396\n",
      "Epoch 90/500, Loss: 6.8552\n",
      "Epoch 100/500, Loss: 6.7730\n",
      "Epoch 110/500, Loss: 6.6929\n",
      "Epoch 120/500, Loss: 6.6148\n",
      "Epoch 130/500, Loss: 6.5388\n",
      "Epoch 140/500, Loss: 6.4646\n",
      "Epoch 150/500, Loss: 6.3923\n",
      "Epoch 160/500, Loss: 6.3219\n",
      "Epoch 170/500, Loss: 6.2531\n",
      "Epoch 180/500, Loss: 6.1861\n",
      "Epoch 190/500, Loss: 6.1208\n",
      "Epoch 200/500, Loss: 6.0572\n",
      "Epoch 210/500, Loss: 5.9952\n",
      "Epoch 220/500, Loss: 5.9348\n",
      "Epoch 230/500, Loss: 5.8760\n",
      "Epoch 240/500, Loss: 5.8187\n",
      "Epoch 250/500, Loss: 5.7629\n",
      "Epoch 260/500, Loss: 5.7086\n",
      "Epoch 270/500, Loss: 5.6556\n",
      "Epoch 280/500, Loss: 5.6040\n",
      "Epoch 290/500, Loss: 5.5537\n",
      "Epoch 300/500, Loss: 5.5047\n",
      "Epoch 310/500, Loss: 5.4569\n",
      "Epoch 320/500, Loss: 5.4104\n",
      "Epoch 330/500, Loss: 5.3649\n",
      "Epoch 340/500, Loss: 5.3206\n",
      "Epoch 350/500, Loss: 5.2774\n",
      "Epoch 360/500, Loss: 5.2352\n",
      "Epoch 370/500, Loss: 5.1939\n",
      "Epoch 380/500, Loss: 5.1537\n",
      "Epoch 390/500, Loss: 5.1143\n",
      "Epoch 400/500, Loss: 5.0759\n",
      "Epoch 410/500, Loss: 5.0383\n",
      "Epoch 420/500, Loss: 5.0016\n",
      "Epoch 430/500, Loss: 4.9656\n",
      "Epoch 440/500, Loss: 4.9304\n",
      "Epoch 450/500, Loss: 4.8960\n",
      "Epoch 460/500, Loss: 4.8623\n",
      "Epoch 470/500, Loss: 4.8292\n",
      "Epoch 480/500, Loss: 4.7969\n",
      "Epoch 490/500, Loss: 4.7652\n",
      "Epoch 500/500, Loss: 4.7341\n",
      "Fold fold-10: Accuracy = 0.1923\n",
      "Average Accuracy across all 10 folds: 0.1906\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "base_path = 'Symbols/classification-task'\n",
    "image_folder = 'Symbols'\n",
    "\n",
    "folds = [f'fold-{i}' for i in range(1, 11)]\n",
    "\n",
    "symbols_df = pd.read_csv('symbols.csv')\n",
    "num_classes = len(symbols_df)\n",
    "\n",
    "input_size = 32 * 32\n",
    "layers = [input_size, 768, 512, num_classes]\n",
    "activation_function = 'tanh'\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "results = []\n",
    "\n",
    "# 10-fold cross-validation\n",
    "for fold in folds:\n",
    "    print(f\"Processing fold: {fold}\")\n",
    "    \n",
    "    #current fold's test set\n",
    "    test_path = os.path.join(base_path, fold, 'test_reduced.csv')\n",
    "    X_test, y_test = load_data(test_path, image_folder)\n",
    "    \n",
    "    # other folds' training data\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    \n",
    "    for other_fold in folds:\n",
    "        if other_fold != fold:  \n",
    "            train_path = os.path.join(base_path, other_fold, 'train_reduced.csv')\n",
    "            X_fold, y_fold = load_data(train_path, image_folder)\n",
    "            X_train_list.append(X_fold)\n",
    "            y_train_list.append(y_fold)\n",
    "    \n",
    "    # Combine all training data\n",
    "    X_train = np.vstack(X_train_list)\n",
    "    y_train = np.concatenate(y_train_list)\n",
    "\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "    X_test_scaled = scaler.transform(X_test_flat)\n",
    "   \n",
    "    model = MLP(\n",
    "        layers=layers,\n",
    "        activation_function=activation_function,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer='batch',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    model.train(X_train_scaled, y_train, epochs=epochs)\n",
    "    \n",
    "    y_pred_classes = model.predict_classes(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    results.append(accuracy)\n",
    "    \n",
    "    print(f'Fold {fold}: Accuracy = {accuracy:.4f}')\n",
    "\n",
    "# average accuracy\n",
    "average_accuracy = sum(results) / len(results)\n",
    "print(f'Average Accuracy across all 10 folds: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the mean and standard deviation tell you about model performance and consistency?\n",
    "- Mean (average accuracy, loss, etc.) indicates the overall performance of the model across multiple runs or dataset. A higher mean typically suggests a better-performing model.\n",
    "- Standard Deviation (SD) measures the variability in performance across different runs. A low SD means the model performs consistently, while a high SD means performance varies significantly across different test cases.\n",
    "- Together, they provide insights into both accuracy and reliability—a high mean with a low SD is ideal because it suggests the model performs well and is stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a high vs. low standard deviation impact confidence in the model’s generalization?\n",
    "- A low standard deviation suggests the model’s performance is stable across different test sets, indicating good generalization to new, unseen data.\n",
    "- A high standard deviation means the model's performance varies a lot, implying it might be overfitting to some subsets of data and underperforming on others. This reduces confidence in its ability to generalize well.\n",
    "- In short, lower SD increases confidence that the model will perform similarly on new data, whereas higher SD raises concerns about inconsistency and possible overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If one configuration has a slightly higher mean accuracy but a significantly higher standard deviation compared to another with marginally lower mean accuracy, which would you choose and why?\n",
    "- If one configuration has a slightly higher mean accuracy but a significantly higher standard deviation compared to another with a marginally lower mean accuracy, the better choice depends on the use case and priorities:\n",
    "- -  If Stability and Reliability Are Critical (like healthcare, finance, autonomous systems), the model with the lower standard deviation, even if its mean accuracy is slightly lower is chosen. A stable model ensures consistent performance across different scenarios, reducing the risk of unpredictable errors.\n",
    "- - If Peak Performance Is the Priority (like recommendation systems, gaming AI), the model with the higher mean accuracy might be preferable, but this choice comes with higher risk—performance could vary significantly across different inputs.\n",
    "- - In general a model with a lower standard deviation is typically preferred.\n",
    "A more consistent model generalizes better to unseen data, making it more reliable in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
