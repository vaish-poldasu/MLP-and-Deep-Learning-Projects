{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 MLP Multi-Class Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.1.2 Model Development from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Activation Functions\n",
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "# MLP Class\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation='relu', lr=0.01, optimizer='sgd'):\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.activation_func = getattr(ActivationFunctions, activation)\n",
    "        self.activation_derivative = getattr(ActivationFunctions, f\"{activation}_derivative\")\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "    \n",
    "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            A = self.activation_func(Z) if i < len(self.weights) - 1 else Z  \n",
    "            activations.append(A)\n",
    "        return activations\n",
    "    \n",
    "    def backward(self, activations, y):\n",
    "        m = y.shape[0]\n",
    "        dA = activations[-1] - y  # Derivative of loss \n",
    "        weight_grads = []\n",
    "        bias_grads = []\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dZ = dA * self.activation_derivative(activations[i+1]) if i < len(self.weights) - 1 else dA\n",
    "            dW = np.dot(activations[i].T, dZ) / m\n",
    "            dB = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            \n",
    "            dA = np.dot(dZ, self.weights[i].T)  # Backpropagate\n",
    "            weight_grads.insert(0, dW)\n",
    "            bias_grads.insert(0, dB)\n",
    "        \n",
    "        return weight_grads, bias_grads\n",
    "    \n",
    "    def update_weights(self, weight_grads, bias_grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lr * weight_grads[i]\n",
    "            self.biases[i] -= self.lr * bias_grads[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            activations = self.forward(X)\n",
    "            weight_grads, bias_grads = self.backward(activations, y)\n",
    "            self.update_weights(weight_grads, bias_grads)\n",
    "            if epoch % 10 == 0:\n",
    "                loss = np.mean((activations[-1] - y) ** 2)\n",
    "                print(f\"Epoch {epoch}: Loss = {loss}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)[-1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(FOLD_PATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m     46\u001b[0m     img_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymbols/images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 48\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     all_train_y\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[0;32m     51\u001b[0m all_train_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_train_y)\n",
      "Cell \u001b[1;32mIn[15], line 20\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(csv_file, img_folder, label_mapping)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Image not found - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping corrupted image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (32, 32)\n",
    "FOLD_PATH = \"Symbols/classification-task\"  \n",
    "\n",
    "def load_data(csv_file, img_folder, label_mapping):\n",
    "    if not os.path.exists(csv_file):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_file}\")\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    required_columns = {'path', 'symbol_id'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"CSV is missing required columns: {required_columns - set(df.columns)}\")\n",
    "\n",
    "    X, y = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.normpath(os.path.join(img_folder, os.path.basename(row['path'])))\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found - {img_path}\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            print(f\"Skipping corrupted image: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.resize(image, IMAGE_SIZE).flatten() / 255.0\n",
    "        X.append(image)\n",
    "        y.append(label_mapping.get(row['symbol_id'], -1))  \n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "#label mapping from symbol IDs\n",
    "def create_label_mapping(symbol_csv):\n",
    "    df = pd.read_csv(symbol_csv)\n",
    "    unique_ids = sorted(df['symbol_id'].unique())\n",
    "    return {symbol_id: idx for idx, symbol_id in enumerate(unique_ids)}\n",
    "\n",
    "symbol_csv = 'symbols.csv'\n",
    "label_mapping = create_label_mapping(symbol_csv)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "all_train_y = []\n",
    "for i in range(1, 11):\n",
    "    csv_path = os.path.join(FOLD_PATH, f\"fold-{i}\", \"train.csv\")  \n",
    "    img_folder = \"Symbols/images\"\n",
    "    \n",
    "    X, y = load_data(csv_path, img_folder, label_mapping)\n",
    "    all_train_y.append(y)\n",
    "\n",
    "all_train_y = np.concatenate(all_train_y)\n",
    "encoder.fit(all_train_y.reshape(-1, 1))\n",
    "\n",
    "fold_accuracies = []\n",
    "for fold in range(1, 11): \n",
    "    print(f\"\\nTraining Fold {fold}...\")\n",
    "\n",
    "    train_X, train_y, test_X, test_y = [], [], [], []\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        csv_path = os.path.join(FOLD_PATH, f\"fold-{i}\", \"train.csv\")  \n",
    "        img_folder = \"Symbols/images\"\n",
    "\n",
    "        X, y = load_data(csv_path, img_folder, label_mapping)\n",
    "        y = encoder.transform(y.reshape(-1, 1)) \n",
    "\n",
    "        if i == fold:  \n",
    "            test_X, test_y = X, y \n",
    "        else:\n",
    "            train_X.append(X)\n",
    "            train_y.append(y)\n",
    "\n",
    "    # Merging all training folds\n",
    "    train_X = np.vstack(train_X)\n",
    "    train_y = np.vstack(train_y)\n",
    "\n",
    "    # Train model\n",
    "    model = MLP(input_size=1024, hidden_layers=[128, 64], output_size=len(label_mapping), activation='relu', lr=0.01)\n",
    "    model.train(train_X, train_y, epochs=30)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.forward(test_X)[-1]\n",
    "    accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(test_y, axis=1))\n",
    "    fold_accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Fold {fold} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#final average accuracy\n",
    "print(f\"\\nFinal Average Accuracy: {np.mean(fold_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_grad_descent(weights, biases, grad_w, grad_b, learning_rate):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= learning_rate * grad_w[i]\n",
    "        biases[i] -= learning_rate * grad_b[i]\n",
    "\n",
    "def batch_grad_descent(weights, biases, grad_w, grad_b, learning_rate):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= learning_rate * np.mean(grad_w[i], axis=0)\n",
    "        biases[i] -= learning_rate * np.mean(grad_b[i], axis=0)\n",
    "\n",
    "def mini_batch_grad_descent(weights, biases, grad_w, grad_b, learning_rate, batch_size):\n",
    "    for i in range(len(weights)):\n",
    "        batch_indices = np.random.choice(len(grad_w[i]), batch_size, replace=False)\n",
    "        weights[i] -= learning_rate * np.mean(grad_w[i][batch_indices], axis=0)\n",
    "        biases[i] -= learning_rate * np.mean(grad_b[i][batch_indices], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_size, activation='sigmoid', optimizer='sgd', learn_rate=0.01,batch_size=32 ):\n",
    "        self.layer_size = layer_size\n",
    "        self.activation_fn = self.activate_func(activation)\n",
    "        self.activation_der = self.activate_der(activation)\n",
    "        self.optimizer=optimizer\n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        for i in range(len(self.layer_size) - 1):\n",
    "            self.weights.append(np.random.randn(self.layer_size[i],self.layer_size[i+1]) *0.01)#to prevent exploding gradients\n",
    "            self.biases.append(np.zeros((1,self.layer_size[i+1])))\n",
    "    \n",
    "    def activate_func(self,name):\n",
    "        if name == 'relu':\n",
    "            return lambda x: np.maximum(0,x)\n",
    "        elif name == 'sigmoid':\n",
    "            return lambda x: 1/(1+np.exp(-x))\n",
    "        elif name == 'tanh':\n",
    "            return lambda x: (np.exp(x) - np.exp(-x))/ (np.exp(x) + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Error-Activation fn\")\n",
    "        \n",
    "    def activate_der(self,name):\n",
    "        if name == 'relu':\n",
    "            return lambda x:np.where(x>0,1,0)\n",
    "        elif name=='sigmoid':\n",
    "            sig = lambda x: 1/(1+np.exp(-x))\n",
    "            return lambda x:sig(x)*(1-sig(x))\n",
    "        elif name == 'tanh':\n",
    "            tan_func = lambda x: (np.exp(x) - np.exp(-x))/ (np.exp(x) + np.exp(-x))\n",
    "            return lambda x: 1- tan_func(x)**2\n",
    "        \n",
    "    def forward_prop(self,X):\n",
    "        activations = [X]\n",
    "        z_sum = []\n",
    "        for w,b in zip(self.weights,self.biases):\n",
    "            z = np.dot(activations[-1],w)+b\n",
    "            z_sum.append(z)\n",
    "            activations.append(self.activation_fn(z))\n",
    "        return activations,z_sum\n",
    "    \n",
    "    def back_prop(self,X,y,activation,z_sum):\n",
    "        m=X.shape[0]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        error = activation[-1] - y\n",
    "        delta = error * self.activate_der(z_sum[-1])\n",
    "\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            grad_w[i] = np.dot(activation[i].T,delta)/m\n",
    "            grad_b[i] = np.sum(delta, axis=0,keepdims=True)/m\n",
    "            if i>0:\n",
    "                delta = np.dot(delta,self.weights[i].T)*self.activate_der(z_sum[i-1])\n",
    "        return grad_w, grad_b\n",
    "    \n",
    "    def update_weights(self,grad_w,grad_b):\n",
    "        if self.optimizer == 'sgd':\n",
    "            stochastic_grad_descent(self.weights,self.biases,grad_w,grad_b,self.learn_rate)\n",
    "        elif self.optimizer == 'batch':\n",
    "            batch_grad_descent(self.weights,self.biases,grad_w,grad_b,self.learn_rate)\n",
    "        elif self.optimizer == 'mini-batch':\n",
    "            mini_batch_grad_descent(self.weights, self.biases,grad_w,grad_b,self.learn_rate,self.batch_size)\n",
    "        else:\n",
    "            raise ValueError(\"Error-optimizer\")\n",
    "        \n",
    "    def train(self,X,y,epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            activations,z_sum = self.forward_prop(X)\n",
    "            grad_w,grad_b = self.back_prop(X,y,activations,z_sum)\n",
    "            self.update_weights(grad_w,grad_b)\n",
    "            if epoch % 10 == 0:\n",
    "                loss = np.mean((activations[-1] - y) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self,X):\n",
    "        activations, _ = self.forward_prop(X)\n",
    "        return activations[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Hyperparameter Tuning & Evaluation with 10-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m accs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m---> 44\u001b[0m     train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_data])\n\u001b[0;32m     46\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_data])\n",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(fold_num)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     21\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_path, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 22\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "train_data = []\n",
    "test_data = []\n",
    "labels = {}\n",
    "\n",
    "symbols = pd.read_csv('symbols.csv')\n",
    "symbol_map = dict(zip(symbols['symbol_id'], symbols['latex']))\n",
    "\n",
    "dataset_path = 'Symbols/classification-task'\n",
    "image_path = 'Symbols/images'\n",
    "\n",
    "def load_data(fold_num):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        fold_path = os.path.join(dataset_path, f'fold-{i}')\n",
    "        df = pd.read_csv(os.path.join(fold_path, 'train.csv'))\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            img_path = os.path.join(image_path, row['path'].split('/')[-1])\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.resize(img, (32, 32)).flatten() / 255.0\n",
    "\n",
    "            if i == fold_num:\n",
    "                test_data.append((img, row['symbol_id']))\n",
    "            else:\n",
    "                train_data.append((img, row['symbol_id']))\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "results = []\n",
    "activations = ['sigmoid', 'relu', 'tanh']\n",
    "optimizers = ['sgd', 'batch', 'mini-batch']\n",
    "layer_sizes = [[1024, 256, 128, 1], [1024, 512, 256, 1]]\n",
    "\n",
    "for activation in activations:\n",
    "    for optimizer in optimizers:\n",
    "        for layer_size in layer_sizes:\n",
    "            accs = []\n",
    "            for i in range(1, 11):\n",
    "                train_data, test_data = load_data(i)\n",
    "                X_train = np.array([i[0] for i in train_data])\n",
    "                y_train = np.array([i[1] for i in train_data])\n",
    "                X_test = np.array([i[0] for i in test_data])\n",
    "                y_test = np.array([i[1] for i in test_data])\n",
    "\n",
    "                model = MLP(layer_size, activation, optimizer)\n",
    "                model.train(X_train, y_train, epochs=100)\n",
    "                y_pred = model.predict(X_test)\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                accs.append(acc)\n",
    "\n",
    "            mean_acc = np.mean(accs)\n",
    "            std_acc = np.std(accs)\n",
    "            results.append((activation, optimizer, layer_size, mean_acc, std_acc))\n",
    "\n",
    "# results\n",
    "results_df = pd.DataFrame(results, columns=['Activation', 'Optimizer', 'Layer_Size', 'Mean_Accuracy', 'Std_Accuracy'])\n",
    "print(results_df.sort_values(by='Mean_Accuracy', ascending=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
